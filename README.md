## Q-Learning
Q-learning is a mode-free RL algorithm, where we aim to approximate the optimal action value function $Q^*(s,a)$, which is the expected return for taking action $a$ in state $s$, and thereafter following the optimal policy. 

Suppose our estimate is defined as $Q(s,a)$ (also referred to as the Q-function). After taking an action and observing the next state $s'$, we calculate the Bellman error as

$$\delta = r + \gamma \max_{a'}Q(s', a') - Q(s, a)$$

where $\gamma$ is the discount factor. The Bellman error tell us the difference between: 
\begin{itemize}
    \item The current Q-value estimate $Q(s, a)$
    \item The new estimate with the immediate reward and discounted future rewards from the next state. This estimate is often referred to as our Bellman targets
\end{itemize}

We update our Q-function as

$$ Q(s,a) \leftarrow Q(s,a) + \alpha \delta $$
where $\alpha$ is the learning rate.

## Deep Q-learning

In Q-learning the action-value function must be estimated across all states and actions. However, in many environments, the state space is too large to store a Q-value for every state action pair. As a result, a function approximator is used 

$$ Q(s,a, \textbf{w}) \approx Q(s,a), $$

where $\textbf{w}$ are the parameters. This allows us to generalise from seen states to unseen states. In Deep Q-learning, a neural network is used as the function approximator, and it is typical to refer to it as a Q-network.  

## Experience Replay

To improve training stability, Deep Q-learning uses an experience replay buffer. The agent stores its experiences in a buffer, which consists of tuples of the form $\{s, a, r, s'\}$. During training a random batch is sampled from the replay buffer, and the parameters of the Q-network are updated. By selecting random samples from our replay buffer, we break the correlation between consecutive samples, stabilizing training.

## Fixed Q-targets

To further stabilize the learning process, Deep Q-Learning makes use of a target network, which is a copy of the Q-network. The target network is responsible for calculating the Bellman targets

$$ y = r + \max_{a'}Q(s', a'; \mathbf{w}^-) $$

where $w^-$ denote the parameters of the target network. These parameters are only updated periodically as copies of the Q-networks parameters.

## Off-Policy
Deep Q-learning is a off-policy RL algorithm, where we find a target policy (the policy that we ultimately want to optimize) using data generated by a different policy, called the behavior policy. The behavior policy is an exploratory policy, whereas target policy is a greedy policy.

The behavior policy used in Q-learning is a epsilon-greedy strategy:
\begin{itemize}
    \item With probability $\epsilon$, choose a random action (exploration).
    \item With probability $1 - \epsilon$, choose the action that maximizes the Q-function (exploitation)
\end{itemize}
During the training process, $\epsilon$ decays to some point. Hence, the agent explores less and exploits more as it learns.

## Training
The Q-network is trained by minimizing the Mean Squared Bellman Error (MSBE)

$$ \mathbb{E}_{\{s, a, r, s'\} \sim P}\left[r + \gamma \max_{a'}Q(s', a'; \mathbf{w}^-) - Q(s, a; \mathbf{w})\right]^2, $$
where $P$ denotes our environment. This expectation unfortunately cannot be computed, because the dynamics of the environment are unknown. As a result, after each step in the environment we take a random sample from the replay buffer to approximate this expectation, and perform one step of gradient descent. 



## Breakout

<div style="display: flex;">

  <div style="flex: 1; text-align: center;">
    <h3>Episode 13500</h3>
    <div style="border: 1px solid black; padding: 5px; display: inline-block">
      <img src="game_results/Breakout/agent_13500.gif" alt="Image 1" style="max-width: 70%; width: 400px;">
    </div>
  </div>

  <div style="flex: 1; text-align: center;">
    <h3>Episode 5000</h3>
    <div style="border: 1px solid black; padding: 5px;; display: inline-block">
      <img src="game_results/Breakout/agent_5000.gif" alt="Image 2" style="max-width: 70%; width: 200px;">
    </div>
  </div>

   <div style="flex: 1; text-align: center;">
    <h3>Episode 10000</h3>
    <div style="border: 1px solid black; padding: 5px;; display: inline-block">
      <img src="game_results/Breakout/agent_10000.gif" alt="Image 3" style="max-width: 70%; width: 200px;">
    </div>
  </div>

</div>
  The following figure shows the total reward the agent achieved for each espisode of training.

<p align="center">
<img src="game_results/Breakout/episode_scores_13500.png" width="800"/>
</p>


## Future Work

## References

## Citation

If you find this code helpful, kindly reference the following:

```
@misc{Zackey2025AtariReinforcementLearning,
  author = {Zackey, Matthew},
  title = {Atari-Reinforcement-Learning},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/MattZackey/Atari-Reinforcement-Learning}},
}
```
